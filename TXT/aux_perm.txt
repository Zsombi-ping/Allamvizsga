Explained as: feature importances

Feature importances, computed as a decrease in score when feature
values are permuted (i.e. become noise). This is also known as 
permutation importance.

If feature importances are computed on the same data as used for training, 
they don't reflect importance of features for generalization. Use a held-out
dataset if you want generalization feature importances.

0.1512 ± 0.0049  Space Indents
0.1411 ± 0.0283  Unique Words
0.0962 ± 0.0144  Function Name Readability
0.0831 ± 0.0226  Tabulators
0.0321 ± 0.0122  Tab Indents
     0 ± 0.0000  Prefer Tabs Over Spaces